{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application: Control games with face position\n",
    "\n",
    "In this notebook, you will learn how to connect the dots between using OpenCV to identify faces within video feeds, and connecting that to PyAutoGUI to drive keyboard inputs in different video game applications. Be sure to test and tinker with the accompanying python script along side this notebook to run the end to end application, as you may not be able to effectively run it from within the notebook itself.\n",
    "\n",
    "The high level steps look like the following:\n",
    "\n",
    "1. Define a webcam capture loop, drawing the \"center bounds\".\n",
    "2. Load and use a pre-made face detection model.\n",
    "3. For any faces found, determine if has moved outside the center-position bounds.\n",
    "4. If out of bounds, send the according key press to the operating system.\n",
    "5. Finally, we can run the script and switch to a window with a game to control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyautogui as gui\n",
    "import time\n",
    "\n",
    "# Set keypress delay to 0\n",
    "gui.PAUSE = 0\n",
    "\n",
    "# Loading the pre-trained face model.\n",
    "model_path = \"../res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "prototxt_path = \"../deploy.prototxt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Webcam loop and drawing the \"center bounds\"\n",
    "\n",
    "To get started, we will create the basic structure for our main gameplay loop. Notice there are placeholder comments for now for the functions that our loop will need to call, which we will add later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(prototxt_path, model_path):\n",
    "    \"\"\" \n",
    "    Run the main loop until cancelled.\n",
    "    \"\"\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(1)\n",
    "    \n",
    "    # Getting the Frame width and height.\n",
    "    frame_width, frame_height = int(cap.get(3)), int(cap.get(4))\n",
    "    \n",
    "    # Co-ordinates of the bounding box on frame\n",
    "    left_x, top_y = frame_width // 2 - 150, frame_height // 2 - 200\n",
    "    right_x, bottom_y = frame_width // 2 + 150, frame_height // 2 + 200\n",
    "    bbox = [left_x, right_x, bottom_y, top_y]\n",
    "    \n",
    "    while not cap.isOpened():\n",
    "        cap = cv2.VideoCapture(1)\n",
    "        \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            return 0\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        # To be added: Detecting and drawing bounding box around faces\n",
    "        \n",
    "        # Drawing the control rectangle in the center of the frame.\n",
    "        frame = cv2.rectangle(frame, (left_x, top_y), (right_x, bottom_y), (0, 0, 255), 5)\n",
    "        # To be added: Checking for game-start position, and checking to run keyboard press.\n",
    "        # Exit the loop on pressing the \"esc\" key.\n",
    "        k = cv2.waitKey(5)\n",
    "        if k == 27:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a pre-made face detection model\n",
    "\n",
    "In order to use pre-trained face model, we will need to do the following steps:\n",
    "\n",
    "1. Load in the deep neural network (DNN).\n",
    "2. Transform an input frame into the require format.\n",
    "3. Set this frame as the input to the face detection model.\n",
    "4. Read out any detected results.\n",
    "\n",
    "---\n",
    "\n",
    "### Reading the DNN model\n",
    "\n",
    "**readNetFromCaffe()** Reads a network model stored in *Caffe* framework's model.\n",
    "\n",
    "#### Function Syntax\n",
    "\n",
    "cv2.dnn.readNetFromCaffe(prototxt[, caffeModel])\n",
    "\n",
    "The function gas **2 required arguments** and 1 optional:\n",
    "\n",
    "1. **prototxt** path to the prototxt file with text description of the network architecture.\n",
    "2. **caffeModel** path to the caffemodel file with learned network.\n",
    "\n",
    "---\n",
    "\n",
    "### Converting an image into the model format\n",
    "\n",
    "**blobFromImage()** Creates 4-dimensional blob from image. Optionally resizes and crops image from center, subtract mean values, scale values by scalefactor and swap Blue and Red Channels.\n",
    "\n",
    "---\n",
    "\n",
    "### Read the neural networl model in main function\n",
    "\n",
    "We will add the following line one at the top of our main **play** function, but outside the loop, so that we only load the model once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to detect the faces in the frame\n",
    "\n",
    "We then need to define the function which runs the detection of faces. We transform the image into blob format with **cv2.dnn.blobFromImage**, assign it as an input into the model using **net.setInput**, and then run the detections using **net.forward()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(net, frame):\n",
    "    \"\"\"\n",
    "    Detect the faces in the frame.\n",
    "    \n",
    "    returns: list of faces in the frame.\n",
    "        here each face is a dictionary of format-\n",
    "        {'start': (startX, startY), 'end': (endX, endY), 'confidence': confidence}\n",
    "    \"\"\"\n",
    "    detected_faces = []\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(frame, (300, 300)),\n",
    "        1.0,\n",
    "        (300, 300),\n",
    "        (104.0, 177.0, 123.0)\n",
    "    )\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.5:\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, 2, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            detected_faces.append(\n",
    "                {\n",
    "                    'start': (startX, startY),\n",
    "                    'end': (endX, endY),\n",
    "                    'confidence': confidence\n",
    "                }\n",
    "            )\n",
    "    return detected_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to draw rectangular bounding box around detected faced.\n",
    "\n",
    "Finally, we want to visually draw a rectangle for each detected face on the screen. This is regardless of whether or not a keyboard signal is to be sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_face(frame, detected_faces):\n",
    "    \"\"\"\n",
    "    Draw rectangular box over detected faces.\n",
    "    \n",
    "    returns: frame with rectangular boxes over detected faces.\n",
    "    \"\"\"\n",
    "    for face in detected_faces:\n",
    "        cv2.rectangle(frame, face[\"start\"], face[\"end\"], (0, 255, 0), 10)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect movement outside the center box\n",
    "\n",
    "This is a function to check that a detected face is inside the bounding box at the center of the frame. If this value is True on one frame and False on the next, then it will tell a feature function that a keyboard press should occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_rect(detected_faces, bbox):\n",
    "    \"\"\"\n",
    "    Check for a detected face inside the bounding box at the center of the frame.\n",
    "    \n",
    "    returns: True or False.\n",
    "    \"\"\"\n",
    "    for face in detected_faces:\n",
    "        x1, y1 = face[\"start\"]\n",
    "        x2, y2 = face[\"end\"]\n",
    "        \n",
    "        if x1 > bbox[0] and x2 < bbox[1]:\n",
    "            if y1 > bbox[3] and y2 < bbox[2]:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send keyboard press on detected movement\n",
    "\n",
    "Based on the output of the *check_rect* function, we can now decide whether to send a keyboard arrow press event to the operating system via PyAutoGUI (imported as *gui*). The *last_mov* check is added to make sure the character doesn't keep drifting in the previous detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(detected_faces, bbox):\n",
    "    \"\"\"\n",
    "    Press correct button depending on the position of detected face and bbox.\n",
    "    \n",
    "    The last_mov check is added for making sure the character doesn't keep\n",
    "    drifting in the previous detection.\n",
    "    \"\"\"\n",
    "    global las_mov\n",
    "    for face in detected_faces:\n",
    "        x1, y1 = face[\"start\"]\n",
    "        x2, y2 = face[\"end\"]\n",
    "        \n",
    "        # Center\n",
    "        if check_rect(detedcted_faces, bbox):\n",
    "            last_mov = \"center\"\n",
    "            return\n",
    "        elif las_mov == \"center\":\n",
    "            # Left\n",
    "            if x1 < bbox[0]:\n",
    "                gui.press(\"left\")\n",
    "                last_mov = \"left\"\n",
    "            # Right\n",
    "            elif x2 > bbox[1]:\n",
    "                gui.press(\"right\")\n",
    "                last_mov = \"right\"\n",
    "            # Down\n",
    "            if y2 > bbox[2]:\n",
    "                gui.press(\"down\")\n",
    "                last_mov = \"down\"\n",
    "            # Up\n",
    "            elif y1 < bbox[3]:\n",
    "                gui.press(\"up\")\n",
    "                last_mov = \"up\"\n",
    "                \n",
    "            # Print out the button pressed if any.\n",
    "            if last_mov != \"center\":\n",
    "                print(last_mov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the play function\n",
    "\n",
    "We can now update our *play* loop to call the functions defined in the prior steps. Below, we have added the calls to detect faces, draw them on the video feed, and then send the according command to PyAutoGUI for keyboard actions. Notice the loop below is also enhanced further with an FPS display, calculated manually using the time elapsed between displayed frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(prototxt_path, model_path):\n",
    "    \"\"\"\n",
    "    Run the main loop until cancelled.\n",
    "    \"\"\"\n",
    "    global last_mov\n",
    "    # Used to record the time when we processed last frame.\n",
    "    prev_frame_time = 0\n",
    "    # Used to record the tme at which we processed current frame.\n",
    "    new_frame_time = 0\n",
    "    \n",
    "    net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    \n",
    "    # Counter for skipping frame.\n",
    "    count = 0\n",
    "    \n",
    "    # Used to initialize the game.\n",
    "    init = 0\n",
    "    \n",
    "    # Getting the Frame width and height.\n",
    "    frame_width, frame_height = int(cap.get(2)), int(cap.get(4))\n",
    "    \n",
    "    # Co-ordinates of the bounding box on frame\n",
    "    left_x, top_y = frame_width // 2 - 150, frame_height // 2 - 200\n",
    "    right_x, bottom_y = frame_width // 2 + 150, frame_height // 2 + 200\n",
    "    bbox = [left_x, right_x, bottom_y, top_y]\n",
    "    \n",
    "    while not cap.isOpened():\n",
    "        cap = cv2.VideoCapture(1)\n",
    "        \n",
    "    while True:\n",
    "        fps = 0\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            return 0\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        # Detect the face.\n",
    "        detected_faces = detect(net, frame)\n",
    "        # Draw bounding box around detected faces.\n",
    "        frame = draw_face(frame, detected_faces)\n",
    "        # Drawing the control rectangle in the center of the frame.\n",
    "        frame = cv2.rectangle(frame, (left_x, top_y), (right_x, bottom_y), (0, 0, 255), 5)\n",
    "        \n",
    "        # Skipping every alternate frame.\n",
    "        if count % 2 == 0:\n",
    "            # For first pass.\n",
    "            if init == 0:\n",
    "                # If face is inside the control rectangle.\n",
    "                if check_rect(detected_faces, bbox):\n",
    "                    init = 1\n",
    "                    cv2.putText(frame, \"Game is running\", (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    cv2.waitKey(10)\n",
    "                    last_mov = \"center\"\n",
    "                    # Click to start the game.\n",
    "                    gui.click(x=500, y=500)\n",
    "            else:\n",
    "                move(detected_faces, bbox)\n",
    "                cv2.waitKey(50)\n",
    "        # Calculating the FPS.\n",
    "        new_frame_time = time.time()\n",
    "        fps = int(1 / (new_frame_time - prev_frame_time))\n",
    "        prev_frame_time = new_frame_time\n",
    "        \n",
    "        frame = cv2.putText(frame, str(fps) + \"FPS\", (200,100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "        cv2.imshow(\"camera_feed\", frame)\n",
    "        count += 1\n",
    "        \n",
    "        # Exit the loop on pressing the 'esc' key.\n",
    "        k = cv2.waitKey(5)\n",
    "        if k == 27:\n",
    "            return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
